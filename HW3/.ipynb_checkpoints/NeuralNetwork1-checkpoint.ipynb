{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " error = 2 * (output - lab_train) / output.shape[0] * self.softmax(params2['Z3'])\n",
    "        change_w['W3'] = np.outer(error,params2['A2'])\n",
    "        \n",
    "        # Calculate b3 update\n",
    "        #error = 2 * (output - lab_train) / output.shape[0] * self.softmax(params2['Z3'])\n",
    "        #change_w['W3'] = np.outer(error,params2['A2'])\n",
    "        \n",
    "        # Calculate W2 update\n",
    "        error = np.dot(params2['W3'].T, error) * self.sigmoidDelta(params2['Z2'])\n",
    "        change_w['W2'] = np.outer(error, params2['A1'])\n",
    "\n",
    "        # Calculate W1 update\n",
    "        error = np.dot(params2['W2'].T, error) * self.sigmoidDelta(params2['Z1'])\n",
    "        change_w['W1'] = np.outer(error, params2['A0'])\n",
    "        \n",
    "    \n",
    "    \n",
    "    # STEP 1: feedforward --------------------------------------------\n",
    "                # z2 = W1.dot(x_i) + b1\n",
    "                a1 = x_i\n",
    "                #print(((params2[\"W1\"]).dot(a1.T)).shape)\n",
    "                z2 = params2[\"W1\"].dot(a1.T) + params2[\"b1\"]\n",
    "                \n",
    "                # a2 = sigmoid(z2)\n",
    "                a2 = self.sigmoid(z2)\n",
    "                \n",
    "                # z3 = W2.dot(a1) + b2\n",
    "                z3 = params2[\"W2\"].dot(a2) + params2[\"b2\"]\n",
    "                \n",
    "                # a3 = sigmoid(z3)\n",
    "                a3 = self.sigmoid(z3)\n",
    "                \n",
    "                # End of feedforward --------------------------------------------\n",
    "                \n",
    "                # STEP 2: backpropagation ----------------------------------------\n",
    "                delta3 = (a3 - y_i)  # error on last layer\n",
    "        \n",
    "                # gradients of last layer\n",
    "                grad_w2 = (delta3.T).dot(a2.T) / batch_size\n",
    "                grad_b2 = np.sum(delta3, axis=0) / batch_size\n",
    "                \n",
    "                # back propagate up to first layer\n",
    "                delta2 = (delta3.dot(params2[\"W2\"])).dot(self.sigmoidPrime(z2))\n",
    "                grad_w1 = (delta2.T).dot(a1) / batch_size\n",
    "                grad_b1 = np.sum(delta2, axis=0) / batch_size\n",
    "  \n",
    "                # regularization of backpropagation\n",
    "                grad_w2 = grad_w2[:, None] + (self.lam * params2[\"W2\"]) / training_size\n",
    "                grad_w1 = grad_w1[:, None] + (self.lam * params2[\"W1\"]) / training_size\n",
    "            \n",
    "                # STEP 3: update weights/ gradient decent -----------------------------------------\n",
    "                params2[\"W1\"] = params2[\"W1\"] - self.alpha * grad_w1\n",
    "                params2[\"b1\"] = params2[\"b1\"] - self.alpha * grad_b1\n",
    "                params2[\"W2\"] = params2[\"W2\"] - self.alpha * grad_w2\n",
    "                params2[\"b2\"] = params2[\"b2\"] - self.alpha * grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNeuralNet():\n",
    "    \n",
    "    def __init__(self, params1, epochs = 10, alpha = 4, beta = 0.9):\n",
    "        self.params1 = params1\n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        #self.lam = lam\n",
    "        self.params2 = self.initialization()\n",
    "    \n",
    "    # initialize all the parameters for use in the neuralnet class\n",
    "    def initialization(self):\n",
    "        # number of nodes in each layer\n",
    "        input_layer = self.params1[0]  # has 784 nodes\n",
    "        hidden_1 = self.params1[1]     # first hidden layer 128\n",
    "        #hidden_2 = self.params1[2]    # second hidden layer 64\n",
    "        output_layer = self.params1[2] # output layer has 10 nodes\n",
    "\n",
    "        params2 = {\n",
    "            'W1'  : np.random.randn(hidden_1, input_layer) * np.sqrt(1. / input_layer),\n",
    "            'W2'  : np.random.randn(output_layer, hidden_1) * np.sqrt(1. / hidden_1),\n",
    "            #'W3'  : np.random.randn(output_layer, hidden_2) * np.sqrt(1./ output_layer),\n",
    "            'b1'  : np.zeros((hidden_1, 1)) * np.sqrt(1. / input_layer),\n",
    "            'b2'  : np.zeros((output_layer, 1)) * np.sqrt(1. / hidden_1),\n",
    "            #'b3'  : np.zeros((output_layer, 1)) * np.sqrt(hidden_1)\n",
    "        }\n",
    "        return params2\n",
    "    \n",
    "    # use sigmoid activagtion function\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    # obtain the derivative for backpropagating\n",
    "    def sigmoidDelta(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exps = np.exp(x - x.max())\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "\n",
    "    def forward_prop(self, x_train, params2):\n",
    "        params2 = self.params2\n",
    "        \n",
    "        activations = {}\n",
    "        \n",
    "        # input layer as our training sample\n",
    "        activations['A0'] = x_train\n",
    "\n",
    "        # input layer to hidden layer 1\n",
    "        activations['Z1'] = np.dot(params2[\"W1\"], activations['A0']) + params2['b1']\n",
    "        activations['A1'] = self.sigmoid(activations['Z1'])\n",
    "\n",
    "        # hidden layer 1 to hidden layer 2\n",
    "        activations['Z2'] = np.dot(params2[\"W2\"], activations['A1']) + params2['b2']\n",
    "        activations['A2'] = self.softmax(activations['Z2'])\n",
    "\n",
    "        # hidden layer 2 to output layer\n",
    "        #params2['Z3'] = np.dot(params2[\"W3\"], params2['A2']) #+ params2['b3']\n",
    "        #params2['A3'] = self.softmax(params2['Z3'])\n",
    "\n",
    "        return activations\n",
    "   \n",
    "    # get updates to the changes of the output of the forward pass\n",
    "    def backward_prop(self, x_train, y_train, params2, activations, m_batch):\n",
    "    \n",
    "        params2 = self.params2\n",
    "\n",
    "        # get the error from the last layer\n",
    "        derivative_Z2 = activations[\"A2\"] - y_train\n",
    "\n",
    "        # get gradients get the last layer\n",
    "        derivative_W2 = (1. / m_batch) * np.dot(derivative_Z2, activations[\"A1\"].T)\n",
    "        derivative_B2 = (1. / m_batch) * np.sum(derivative_Z2, axis = 1, keepdims = True)\n",
    "\n",
    "        # then back propagate it to the first layer\n",
    "        derivative_A1 = np.dot(params2[\"W2\"].T, derivative_Z2)\n",
    "        derivative_Z1 = derivative_A1 * self.sigmoidDelta(activations['Z1'])\n",
    "\n",
    "        # get gradients at the first layer\n",
    "        derivative_W1 = (1. / m_batch) * np.dot(derivative_Z1, x_train.T)\n",
    "        derivative_B1 = (1. / m_batch) * np.sum(derivative_Z1, axis = 1, keepdims = True)\n",
    "\n",
    "        gradients = {\"DW1\" : derivative_W1, \"DB1\" : derivative_B1, \"DW2\" : derivative_W2, \"DB2\" : derivative_B2}\n",
    "\n",
    "        return gradients  \n",
    "    \n",
    "    # do a cross_entropy loss\n",
    "    def compute_loss(y, y_hat):\n",
    "        loss_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "        k = y.shape[1]\n",
    "        loss = -(1. / k) * loss_sum\n",
    "        return loss\n",
    "\n",
    "    def update_network_parameters(self, changes_to_w):\n",
    "        \n",
    "        for key, value in changes_to_w.items():\n",
    "            self.params2[key] = self.params2[key] - (self.alpha * value)\n",
    "            #self.params2[key] -= self.alpha * value\n",
    "\n",
    "    def compute_accuracy(self, x_valid, y_valid):\n",
    "        predictions = []\n",
    "\n",
    "        for x, y in zip(x_valid, y_valid):\n",
    "            output = self.forward_pass(x)\n",
    "            pred = np.argmax(output)\n",
    "            predictions.append(pred == np.argmax(y))\n",
    "        \n",
    "        return np.mean(predictions)\n",
    "\n",
    "    def train(self, x_train, y_train, x_valid, y_valid):\n",
    "        \n",
    "        params2 = self.params2\n",
    "        start_time = time.time() \n",
    "        batch_size = 128\n",
    "        size_t = 60000\n",
    "        batches = -(-size_t // batch_size)\n",
    "        \n",
    "        # initialize for momentum to keep a moving \n",
    "        # average of our gradients\n",
    "        grad_w1 = np.zeros(params2[\"W1\"].shape)\n",
    "        grad_b1 = np.zeros(params2[\"b1\"].shape)\n",
    "        grad_w2 = np.zeros(params2[\"W2\"].shape)\n",
    "        grad_b2 = np.zeros(params2[\"b2\"].shape)\n",
    "\n",
    "        for iteration in range(self.epochs):\n",
    "            \n",
    "            # shuffle dataset\n",
    "            permutation = np.random.permutation(x_train.shape[1])\n",
    "            x_i = x_train[:, permutation]\n",
    "            y_i = y_train[:, permutation]\n",
    "    \n",
    "            # shuffle training set\n",
    "            #images_train_shuffled, labels_train_shuffled = shuffleData(images_train, labels_train)\n",
    "            #batches = [(images_train_shuffled[i:i + batch_size], labels_train_shuffled[i:i + batch_size])\n",
    "                           #for i in range(0, training_size, batch_size)]\n",
    "            \n",
    "            for batch in range(batches):\n",
    "            #for x_i, y_i in zip(images_train, labels_train):\n",
    "                #x_i = batch[0]\n",
    "                #y_i = batch[1]\n",
    "                \n",
    "                begin = batch * batch_size\n",
    "                end = min(begin + batch_size, x_train.shape[1] - 1)\n",
    "                X = x_i[:, begin:end]\n",
    "                Y = y_i[:, begin:end]\n",
    "                m_batch = end - begin\n",
    "                \n",
    "                # STEP 1: feedforward --------------------------------------------\n",
    "                activations = self.forward_prop(x_i, params2)\n",
    "                \n",
    "                # STEP 2: backpropagation ----------------------------------------\n",
    "                gradients = self.backward_prop(x_i, y_i, params2, activations, m_batch)\n",
    "                \n",
    "                # Momentum step ----------------------------------------\n",
    "                grad_w1 = (self.beta * grad_w1 + (1. - self.beta) * gradients['DW1'])\n",
    "                grad_b1 = (self.beta * grad_b1 + (1. - self.beta) * gradients['DB1'])\n",
    "                grad_w2 = (self.beta * grad_w2 + (1. - self.beta) * gradients['DW2'])\n",
    "                grad_b2 = (self.beta * grad_b2 + (1. - self.beta) * gradients['DB2'])\n",
    "                \n",
    "                # STEP 3: update weights/ gradient decent -----------------------------------------\n",
    "                params2[\"W1\"] = params2[\"W1\"] - self.alpha * grad_w1\n",
    "                params2[\"b1\"] = params2[\"b1\"] - self.alpha * grad_b1\n",
    "                params2[\"W2\"] = params2[\"W2\"] - self.alpha * grad_w2\n",
    "                params2[\"b2\"] = params2[\"b2\"] - self.alpha * grad_b2\n",
    "                \n",
    "        activations = self.forward_prop(x_train, params2)\n",
    "        train_cost  = self.compute_loss(y_train, activations[\"A2\"])\n",
    "        activations = self.forward_prop(x_valid, params2)\n",
    "        test_cost   = self.compute_loss(y_valid, activations[\"A2\"])  \n",
    "        \n",
    "        #accuracy = self.compute_accuracy(x_valid, y_valid)\n",
    "        print('Epoch: {0}, Time Spent: {1:.2f}s, train cost: {2:.2f}, test cost: {2:.2f}%'.format(\n",
    "            iteration + 1, time.time() - start_time, train_cost, test_cost\n",
    "        ))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "images = pd.read_csv(\"mnist_image.csv\", header = None)\n",
    "labels = pd.read_csv(\"mnist_label.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to arrays\n",
    "images = np.asarray(images)\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize image data\n",
    "images = np.multiply(images, 1.0 / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do one-hot encoding for our labels\n",
    "# e.g 7 => [0 0 0 0 0 0 0 1 0 0]\n",
    "digits = 10\n",
    "examples = labels.shape[0]\n",
    "labels = labels.reshape(1, examples)\n",
    "labels_new = np.eye(digits)[labels.astype('int32')]\n",
    "labels_new = labels_new.T.reshape(digits, examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and validation\n",
    "size_train = 60000\n",
    "size_test = images.shape[0] - size_train\n",
    "images_train, images_valid = images[:size_train].T, images[size_train:].T\n",
    "labels_train, labels_valid = labels_new[:, :size_train], labels_new[:, size_train:]\n",
    "\n",
    "#shuffle the data\n",
    "shufl_idx = np.random.permutation(size_train)\n",
    "images_train, labels_train = images_train[:, shufl_idx], labels_train[:, shufl_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(784, 10000)\n",
      "(10, 60000)\n",
      "(10, 10000)\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(images_train.shape)\n",
    "print(images_valid.shape)\n",
    "print(labels_train.shape)\n",
    "print(labels_valid.shape)\n",
    "print(labels_train.T[0][0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "training_size = len(images_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\moras\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:31: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-239-57ddbccae2e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mneuralnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyNeuralNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# params = [input_size, hidden_1, hidden_2, output_size]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mneuralnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-238-200ee90f9395>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x_train, y_train, x_valid, y_valid)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m                 \u001b[1;31m# STEP 1: feedforward --------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m                 \u001b[0mactivations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m                 \u001b[1;31m# STEP 2: backpropagation ----------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-238-200ee90f9395>\u001b[0m in \u001b[0;36mforward_prop\u001b[1;34m(self, x_train, params2)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# input layer to hidden layer 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Z1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'A0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mparams2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'A1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Z1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "neuralnet = myNeuralNet(params1 = [784, 64, 10]) # params = [input_size, hidden_1, hidden_2, output_size]\n",
    "neuralnet.train(images_train, labels_train, images_valid, labels_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
