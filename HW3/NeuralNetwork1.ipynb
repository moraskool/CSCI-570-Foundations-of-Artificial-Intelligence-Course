{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " error = 2 * (output - lab_train) / output.shape[0] * self.softmax(params2['Z3'])\n",
    "        change_w['W3'] = np.outer(error,params2['A2'])\n",
    "        \n",
    "        # Calculate b3 update\n",
    "        #error = 2 * (output - lab_train) / output.shape[0] * self.softmax(params2['Z3'])\n",
    "        #change_w['W3'] = np.outer(error,params2['A2'])\n",
    "        \n",
    "        # Calculate W2 update\n",
    "        error = np.dot(params2['W3'].T, error) * self.sigmoidDelta(params2['Z2'])\n",
    "        change_w['W2'] = np.outer(error, params2['A1'])\n",
    "\n",
    "        # Calculate W1 update\n",
    "        error = np.dot(params2['W2'].T, error) * self.sigmoidDelta(params2['Z1'])\n",
    "        change_w['W1'] = np.outer(error, params2['A0'])\n",
    "        \n",
    "    \n",
    "    \n",
    "    # STEP 1: feedforward --------------------------------------------\n",
    "                # z2 = W1.dot(x_i) + b1\n",
    "                a1 = x_i\n",
    "                #print(((params2[\"W1\"]).dot(a1.T)).shape)\n",
    "                z2 = params2[\"W1\"].dot(a1.T) + params2[\"b1\"]\n",
    "                \n",
    "                # a2 = sigmoid(z2)\n",
    "                a2 = self.sigmoid(z2)\n",
    "                \n",
    "                # z3 = W2.dot(a1) + b2\n",
    "                z3 = params2[\"W2\"].dot(a2) + params2[\"b2\"]\n",
    "                \n",
    "                # a3 = sigmoid(z3)\n",
    "                a3 = self.sigmoid(z3)\n",
    "                \n",
    "                # End of feedforward --------------------------------------------\n",
    "                \n",
    "                # STEP 2: backpropagation ----------------------------------------\n",
    "                delta3 = (a3 - y_i)  # error on last layer\n",
    "        \n",
    "                # gradients of last layer\n",
    "                grad_w2 = (delta3.T).dot(a2.T) / batch_size\n",
    "                grad_b2 = np.sum(delta3, axis=0) / batch_size\n",
    "                \n",
    "                # back propagate up to first layer\n",
    "                delta2 = (delta3.dot(params2[\"W2\"])).dot(self.sigmoidPrime(z2))\n",
    "                grad_w1 = (delta2.T).dot(a1) / batch_size\n",
    "                grad_b1 = np.sum(delta2, axis=0) / batch_size\n",
    "  \n",
    "                # regularization of backpropagation\n",
    "                grad_w2 = grad_w2[:, None] + (self.lam * params2[\"W2\"]) / training_size\n",
    "                grad_w1 = grad_w1[:, None] + (self.lam * params2[\"W1\"]) / training_size\n",
    "            \n",
    "                # STEP 3: update weights/ gradient decent -----------------------------------------\n",
    "                params2[\"W1\"] = params2[\"W1\"] - self.alpha * grad_w1\n",
    "                params2[\"b1\"] = params2[\"b1\"] - self.alpha * grad_b1\n",
    "                params2[\"W2\"] = params2[\"W2\"] - self.alpha * grad_w2\n",
    "                params2[\"b2\"] = params2[\"b2\"] - self.alpha * grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNeuralNet():\n",
    "    \n",
    "    def __init__(self, params1, epochs = 10, alpha = 0.9, beta = 0.9):\n",
    "        self.params1 = params1\n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.params2 = self.initialization()\n",
    "    \n",
    "    # initialize all the parameters for use in the neuralnet class\n",
    "    def initialization(self):\n",
    "        # number of nodes in each layer\n",
    "        input_layer = self.params1[0]  # input layer has 784 nodes\n",
    "        hidden_1 = self.params1[1]     # first hidden layer 128\n",
    "        hidden_2 = self.params1[2]     # second hidden layer 64\n",
    "        output_layer = self.params1[3] # output layer has 10 nodes\n",
    "\n",
    "        params2 = {\n",
    "            'W1'  : np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
    "            'W2'  : np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
    "            'W3'  : np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer),\n",
    "            'b1'  : np.zeros((hidden_1, 1)) * np.sqrt(1. / input_layer),\n",
    "            'b2'  : np.zeros((hidden_2, 1)) * np.sqrt(1. / hidden_1),\n",
    "            'b3'  : np.zeros((output_layer, 1)) * np.sqrt(hidden_2)\n",
    "        }\n",
    "        return params2\n",
    "    \n",
    "    # use sigmoid activagtion function\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0/(1 + np.exp(-x))\n",
    "\n",
    "    # obtain the derivative for backpropagating\n",
    "    def sigmoidDelta(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "    \n",
    "    # use a deactivation function\n",
    "    def softmax(self, x):\n",
    "        exps = np.exp(x - x.max())\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "     \n",
    "    # get its derivative for back propagating\n",
    "    def softmaxDelta(self, x):\n",
    "        exps = np.exp(x - x.max())\n",
    "        return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
    "    \n",
    "    \n",
    "    def forward_prop(self, x_train):\n",
    "        \n",
    "        params2 = self.params2\n",
    "        \n",
    "        # input layer as our training sample\n",
    "        params2['A0'] = x_train\n",
    "\n",
    "        # input layer to hidden layer 1\n",
    "        params2['Z1'] = np.dot(params2[\"W1\"], activations['A0']) #+ #params2['b1']\n",
    "        params2['A1'] = self.sigmoid(activations['Z1'])\n",
    "\n",
    "        # hidden layer 1 to hidden layer 2\n",
    "        params2['Z2'] = np.dot(params2[\"W2\"], activations['A1']) #+ #params2['b2']\n",
    "        params2['A2'] = self.softmax(activations['Z2'])\n",
    "\n",
    "        # hidden layer 2 to output layer\n",
    "        params2['Z3'] = np.dot(params2[\"W3\"], params2['A2']) #+ params2['b3']\n",
    "        params2['A3'] = self.softmax(params2['Z3'])\n",
    "\n",
    "        return params2['A2']\n",
    "   \n",
    "    # get updates to the changes of the output of the forward pass\n",
    "    def backward_prop(self, y_train, activations, m_batch):\n",
    "    \n",
    "        params2 = self.params2\n",
    "\n",
    "        # get the error from the last layer\n",
    "        derivative_Z2 = activations[\"A2\"] - y_train\n",
    "\n",
    "        # get gradients get the last layer\n",
    "        derivative_W2 = (1. / m_batch) * np.dot(derivative_Z2, activations[\"A1\"].T)\n",
    "        derivative_B2 = (1. / m_batch) * np.sum(derivative_Z2, axis = 1, keepdims = True)\n",
    "\n",
    "        # then back propagate it to the first layer\n",
    "        derivative_A1 = np.dot(params2[\"W2\"].T, derivative_Z2)\n",
    "        derivative_Z1 = derivative_A1 * self.sigmoidDelta(activations['Z1'])\n",
    "\n",
    "        # get gradients at the first layer\n",
    "        derivative_W1 = (1. / m_batch) * np.dot(derivative_Z1, x_train.T)\n",
    "        derivative_B1 = (1. / m_batch) * np.sum(derivative_Z1, axis = 1, keepdims = True)\n",
    "\n",
    "        gradients = {\"DW1\" : derivative_W1, \"DB1\" : derivative_B1, \"DW2\" : derivative_W2, \"DB2\" : derivative_B2}\n",
    "\n",
    "        return gradients  \n",
    "\n",
    "    # compute the accuracy of the validation tests\n",
    "    def calculate_accuracy(self, x_valid, y_valid):\n",
    "        validations = []\n",
    "        \n",
    "        for x, y in zip(x_valid, y_valid):\n",
    "            \n",
    "            output = self.forward_pass(x)\n",
    "            validate = np.argmax(output)\n",
    "            validations.append(validate == np.argmax(y))\n",
    "            #pd.DataFrame(validate).to_csv('test_predictions.csv',index = False)\n",
    "        return np.mean(validations)\n",
    "\n",
    "    def train(self, x_train, y_train, x_valid, y_valid):\n",
    "        \n",
    "        params2 = self.params2\n",
    "        start_time = time.time() \n",
    "        \n",
    "        # initialize for momentum to keep a moving \n",
    "        # average of our gradients\n",
    "        grad_w1 = np.zeros(params2[\"W1\"].shape)\n",
    "        grad_b1 = np.zeros(params2[\"b1\"].shape)\n",
    "        grad_w2 = np.zeros(params2[\"W2\"].shape)\n",
    "        grad_b2 = np.zeros(params2[\"b2\"].shape)\n",
    "        grad_w3 = np.zeros(params2[\"W3\"].shape)\n",
    "        grad_b3 = np.zeros(params2[\"b2\"].shape)\n",
    "\n",
    "        for iteration in range(self.epochs):\n",
    "            \n",
    "            \n",
    "            for x_i, y_i in zip(x_train, y_train):\n",
    "            \n",
    "                # STEP 1: feedforward --------------------------------------------\n",
    "                activations = self.forward_prop(x_i, params2)\n",
    "                \n",
    "                # STEP 2: backpropagation ----------------------------------------\n",
    "                gradients = self.backward_prop(y_i, activations)\n",
    "                \n",
    "                # Momentum step ----------------------------------------\n",
    "                grad_w1 = (self.beta * grad_w1 + (1. - self.beta) * gradients['W1'])\n",
    "                grad_w2 = (self.beta * grad_w2 + (1. - self.beta) * gradients['W2'])\n",
    "                grad_w3 = (self.beta * grad_w3 + (1. - self.beta) * gradients['W3'])\n",
    "                \n",
    "                #grad_b1 = (self.beta * grad_b1 + (1. - self.beta) * gradients['b1'])\n",
    "                #grad_b2 = (self.beta * grad_b2 + (1. - self.beta) * gradients['b2'])\n",
    "                #grad_b3 = (self.beta * grad_b3 + (1. - self.beta) * gradients['b3'])\n",
    "                \n",
    "                # STEP 3: update weights/ gradient decent -----------------------------------------\n",
    "                params2[\"W1\"] = params2[\"W1\"] - self.alpha * grad_w1\n",
    "                params2[\"W2\"] = params2[\"W2\"] - self.alpha * grad_w2\n",
    "                params2[\"W3\"] = params2[\"W3\"] - self.alpha * grad_w3\n",
    "        \n",
    "        accuracy = self.calculate_accuracy(x_valid, y_valid)\n",
    "        print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
    "            iteration + 1, time.time() - start_time, accuracy * 100))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mnist_image.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0e36fb3b5581>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mnist_image.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mnist_label.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\moras\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\moras\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\moras\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\moras\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\moras\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mnist_image.csv'"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "images = pd.read_csv(\"mnist_image.csv\", header = None)\n",
    "labels = pd.read_csv(\"mnist_label.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to arrays\n",
    "images = np.asarray(images)\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize image data\n",
    "images = np.multiply(images, 1.0 / 255.0)\n",
    "images = images.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do one-hot encoding for our labels\n",
    "# e.g 7 => [0 0 0 0 0 0 0 1 0 0]\n",
    "digits = 10\n",
    "examples = labels.shape[0]\n",
    "labels = labels.reshape(1, examples)\n",
    "labels_new = np.eye(digits)[labels.astype('int32')]\n",
    "labels_new = labels_new.T.reshape(digits, examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and validation\n",
    "size_train = 60000\n",
    "size_test = images.shape[0] - size_train\n",
    "images_train, images_valid = images[:size_train], images[size_train:]\n",
    "labels_train, labels_valid = labels_new[:,:size_train], labels_new[:, size_train:]\n",
    "labels_train = labels_train.T\n",
    "labels_valid = labels_valid.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a576ddfdef52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m138\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mneuralnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyNeuralNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# params = [input_size, hidden_1, hidden_2, output_size]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mneuralnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--- %s seconds ---\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "np.random.seed(138)\n",
    "neuralnet = myNeuralNet(sizes=[784, 128, 64, 10]) # params = [input_size, hidden_1, hidden_2, output_size]\n",
    "neuralnet.train(images_train, labels_train, images_valid, labels_valid)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
